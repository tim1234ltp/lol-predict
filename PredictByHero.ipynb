{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "\n",
    "This is the notebook for building PredictionByHero model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team1Win</th>\n",
       "      <th>t1_1</th>\n",
       "      <th>t1_10</th>\n",
       "      <th>t1_101</th>\n",
       "      <th>t1_102</th>\n",
       "      <th>t1_103</th>\n",
       "      <th>t1_104</th>\n",
       "      <th>t1_105</th>\n",
       "      <th>t1_106</th>\n",
       "      <th>t1_107</th>\n",
       "      <th>...</th>\n",
       "      <th>t2_85</th>\n",
       "      <th>t2_86</th>\n",
       "      <th>t2_89</th>\n",
       "      <th>t2_9</th>\n",
       "      <th>t2_90</th>\n",
       "      <th>t2_91</th>\n",
       "      <th>t2_92</th>\n",
       "      <th>t2_96</th>\n",
       "      <th>t2_98</th>\n",
       "      <th>t2_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95892</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135581</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60975</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26646</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133570</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100498</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45465</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95029</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128637</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53080</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        team1Win  t1_1  t1_10  t1_101  t1_102  t1_103  t1_104  t1_105  t1_106  \\\n",
       "95892        1.0   0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "135581      -1.0   0.0    0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "60975        1.0   0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "26646        1.0   0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "133570      -1.0   0.0    0.0     0.0     0.0     0.0     1.0     1.0     0.0   \n",
       "100498      -1.0   0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "45465        1.0   0.0    1.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "95029       -1.0   0.0    0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "128637       1.0   0.0    0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "53080        1.0   0.0    0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "\n",
       "        t1_107  ...    t2_85  t2_86  t2_89  t2_9  t2_90  t2_91  t2_92  t2_96  \\\n",
       "95892      0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "135581     0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "60975      0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "26646      0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "133570     0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "100498     0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "45465      0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "95029      0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "128637     0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "53080      0.0  ...      0.0    0.0      0     0      0      0    0.0      0   \n",
       "\n",
       "        t2_98  t2_99  \n",
       "95892       0      0  \n",
       "135581      0      0  \n",
       "60975       0      0  \n",
       "26646       0      0  \n",
       "133570      0      0  \n",
       "100498      0      0  \n",
       "45465       0      0  \n",
       "95029       0      0  \n",
       "128637      0      0  \n",
       "53080       0      0  \n",
       "\n",
       "[10 rows x 273 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load and randomise data\n",
    "dataset = pd.read_csv('heroSelect.csv', index_col = 0)\n",
    "dataset = dataset.take(np.random.permutation(len(dataset)))\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset (183452, 273)\n",
      "train: (148595, 273) validation: (18346, 273) test: (16511, 273)\n"
     ]
    }
   ],
   "source": [
    "print('dataset', dataset.shape)\n",
    "dataset, validation = train_test_split(dataset, test_size = 0.1)\n",
    "train, test = train_test_split(dataset, test_size = 0.1)\n",
    "print('train:', train.shape, 'validation:', validation.shape, 'test:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input/output placeholders\n",
    "x_team1    = tf.placeholder(\"float\", shape=[None, 136], name='x_dire')\n",
    "x_team2 = tf.placeholder(\"float\", shape=[None, 136], name='x_radiant')\n",
    "y_true = tf.placeholder(\"float\", shape=[None, 2], name='y_true')\n",
    "\n",
    "#we'll use dropout layers for regularisation which need a keep probability\n",
    "keep_prob1 = tf.placeholder(\"float\", name='keep_prob1')\n",
    "keep_prob2 = tf.placeholder(\"float\", name='keep_prob2')\n",
    "\n",
    "#there doesn't seem to be any other way to differenciate train and validation summaries for TensorBoard\n",
    "loss_name     = tf.placeholder(\"string\", name='loss_name')\n",
    "accuracy_name = tf.placeholder(\"string\", name='accuracy_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight init for fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_weight_bias(in_size, out_size):\n",
    "    initial_weight = tf.truncated_normal([in_size, out_size], stddev=0.2, mean=0.0)\n",
    "    initial_bias = tf.constant(0.1, shape=[out_size])\n",
    "    return tf.Variable(initial_weight), tf.Variable(initial_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"hero_layers_1\") as scope:\n",
    "    W_hero1, b_hero1 = fc_weight_bias(136,80)      \n",
    "    #note that team1 layer and team2 layer use the same weights and biases\n",
    "    team1_layer1 = tf.nn.relu(tf.matmul(x_team1, W_hero1) + b_hero1)\n",
    "    team2_layer1 = tf.nn.relu(tf.matmul(x_team2, W_hero1) + b_hero1)\n",
    "\n",
    "#second hero layer\n",
    "with tf.name_scope(\"hero_layers_2\") as scope:    \n",
    "    W_hero2, b_hero2 = fc_weight_bias(80,80)    \n",
    "    #again, team1 and team2 use the same weights and biases\n",
    "    team1_layer2 = tf.nn.relu(tf.matmul(team1_layer1, W_hero2) + b_hero2)\n",
    "    team2_layer2 = tf.nn.relu(tf.matmul(team2_layer1, W_hero2) + b_hero2)\n",
    "\n",
    "#now concatenate the team1 and team2 team outputs\n",
    "with tf.name_scope(\"hero_layers_concat\") as scope:\n",
    "    team1_team2_concat = tf.concat([team1_layer2, team2_layer2], 1)\n",
    "    team1_team2_drop = tf.nn.dropout(team1_team2_concat, keep_prob1)\n",
    "    h_drop1 = tf.nn.dropout(team1_team2_drop, keep_prob1)\n",
    "\n",
    "with tf.name_scope(\"hidden_layer_1\") as scope:\n",
    "    W_hidden1, b_hidden1 = fc_weight_bias(160,120)    \n",
    "    h_hidden1 = tf.nn.relu(tf.matmul(h_drop1, W_hidden1) + b_hidden1)\n",
    "    h_drop2 = tf.nn.dropout(h_hidden1, keep_prob2)\n",
    "\n",
    "with tf.name_scope(\"hidden_layer_2\") as scope:\n",
    "    W_hidden2, b_hidden2 = fc_weight_bias(120,75)    \n",
    "    h_hidden2 = tf.nn.relu(tf.matmul(h_drop2, W_hidden2) + b_hidden2)\n",
    "\n",
    "with tf.name_scope(\"hidden_layer_3\") as scope:\n",
    "    W_hidden3, b_hidden3 = fc_weight_bias(75,25)    \n",
    "    h_hidden3 = tf.nn.relu(tf.matmul(h_hidden2, W_hidden3) + b_hidden3)\n",
    "\n",
    "with tf.name_scope(\"output_layer\") as scope:\n",
    "    W_hidden4, b_hidden4 = fc_weight_bias(25,2)    \n",
    "    y_pred = tf.nn.softmax(tf.matmul(h_hidden3, W_hidden4) + b_hidden4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss_calculations\") as scope:\n",
    "    cross_entropy = -tf.reduce_sum(y_true * tf.log(y_pred + 1e-8))\n",
    "    weights_sum   = tf.add_n([tf.nn.l2_loss(variable) for variable in tf.global_variables()])\n",
    "    mean_loss     = cross_entropy + weights_sum\n",
    "    loss          = tf.reduce_mean(mean_loss)\n",
    "\n",
    "with tf.name_scope(\"trainer\") as scope:\n",
    "    train_step    = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy_calculations\") as scope:\n",
    "    correct  = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFeed Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_feed(dataset, kp1=1.0, kp2=1.0, loss_str='loss', accuracy_str='accuracy'):\n",
    "    team1_data, team2_data = dataset.ix[:,1:137], dataset.ix[:,137:273]\n",
    "    winners = pd.get_dummies(dataset['team1Win'])\n",
    "    return {\n",
    "        x_team1: team1_data,\n",
    "        x_team2: team2_data,\n",
    "        y_true: winners,\n",
    "        loss_name: loss_str,\n",
    "        accuracy_name: accuracy_str,\n",
    "        keep_prob1: kp1,\n",
    "        keep_prob2: kp2\n",
    "    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_feed      = get_data_feed(train,      loss_str = 'loss_train',      accuracy_str = 'accuracy_train')\n",
    "validation_feed = get_data_feed(validation, loss_str = 'loss_validation', accuracy_str = 'accuracy_validation')\n",
    "test_feed       = get_data_feed(test,       loss_str = 'loss_test',       accuracy_str = 'accuracy_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_size=1500): #1500 is about 1% of the entire training sets\n",
    "    #randomise before every epoch\n",
    "    dataset = dataset.take(np.random.permutation(len(dataset)))\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(dataset):\n",
    "        yield dataset[i : i + batch_size]\n",
    "        i = i + batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 122231, train: 0.511141, validation: 0.51052\n",
      "epoch 1, loss: 110933, train: 0.510953, validation: 0.511719\n",
      "epoch 2, loss: 106668, train: 0.510495, validation: 0.512264\n",
      "epoch 3, loss: 105162, train: 0.508644, validation: 0.509811\n",
      "epoch 4, loss: 104429, train: 0.507493, validation: 0.512319\n",
      "epoch 5, loss: 104167, train: 0.506679, validation: 0.508503\n",
      "epoch 6, loss: 104018, train: 0.50538, validation: 0.508503\n",
      "epoch 7, loss: 103928, train: 0.504687, validation: 0.508776\n",
      "epoch 8, loss: 103879, train: 0.503476, validation: 0.506595\n",
      "epoch 9, loss: 103833, train: 0.503752, validation: 0.507468\n",
      "epoch 10, loss: 103802, train: 0.504108, validation: 0.507631\n",
      "epoch 11, loss: 103782, train: 0.503335, validation: 0.508776\n",
      "epoch 12, loss: 103759, train: 0.503092, validation: 0.508558\n",
      "epoch 13, loss: 103731, train: 0.503462, validation: 0.507522\n",
      "epoch 14, loss: 103713, train: 0.50322, validation: 0.507413\n",
      "epoch 15, loss: 103691, train: 0.503826, validation: 0.509103\n",
      "epoch 16, loss: 103669, train: 0.504243, validation: 0.509212\n",
      "epoch 17, loss: 103655, train: 0.503954, validation: 0.508285\n",
      "epoch 18, loss: 103634, train: 0.504445, validation: 0.507141\n",
      "epoch 19, loss: 103614, train: 0.505367, validation: 0.507577\n",
      "epoch 20, loss: 103597, train: 0.505179, validation: 0.507686\n",
      "epoch 21, loss: 103581, train: 0.506047, validation: 0.507086\n",
      "epoch 22, loss: 103568, train: 0.506175, validation: 0.504688\n",
      "epoch 23, loss: 103552, train: 0.506619, validation: 0.503597\n",
      "epoch 24, loss: 103542, train: 0.505878, validation: 0.505669\n",
      "epoch 25, loss: 103524, train: 0.506773, validation: 0.506595\n",
      "epoch 26, loss: 103511, train: 0.507096, validation: 0.504851\n",
      "epoch 27, loss: 103496, train: 0.5068, validation: 0.507522\n",
      "epoch 28, loss: 103480, train: 0.506498, validation: 0.506813\n",
      "epoch 29, loss: 103466, train: 0.506821, validation: 0.505778\n",
      "epoch 30, loss: 103451, train: 0.507682, validation: 0.506541\n",
      "epoch 31, loss: 103439, train: 0.507702, validation: 0.506704\n",
      "epoch 32, loss: 103425, train: 0.508489, validation: 0.507086\n",
      "epoch 33, loss: 103414, train: 0.508274, validation: 0.509103\n",
      "epoch 34, loss: 103400, train: 0.509385, validation: 0.507631\n",
      "epoch 35, loss: 103388, train: 0.509519, validation: 0.508067\n",
      "epoch 36, loss: 103377, train: 0.509136, validation: 0.509321\n",
      "epoch 37, loss: 103366, train: 0.508436, validation: 0.508776\n",
      "epoch 38, loss: 103355, train: 0.508523, validation: 0.509266\n",
      "epoch 39, loss: 103343, train: 0.509479, validation: 0.510029\n",
      "epoch 40, loss: 103329, train: 0.510333, validation: 0.511174\n",
      "epoch 41, loss: 103316, train: 0.510031, validation: 0.512864\n",
      "epoch 42, loss: 103303, train: 0.511417, validation: 0.511937\n",
      "epoch 43, loss: 103297, train: 0.509613, validation: 0.509702\n",
      "epoch 44, loss: 103283, train: 0.510421, validation: 0.512046\n",
      "epoch 45, loss: 103272, train: 0.510656, validation: 0.511447\n",
      "epoch 46, loss: 103262, train: 0.510556, validation: 0.508939\n",
      "epoch 47, loss: 103247, train: 0.51135, validation: 0.511065\n",
      "epoch 48, loss: 103235, train: 0.511881, validation: 0.510684\n",
      "epoch 49, loss: 103224, train: 0.511693, validation: 0.510302\n",
      "epoch 50, loss: 103215, train: 0.511827, validation: 0.509266\n",
      "epoch 51, loss: 103207, train: 0.511787, validation: 0.508721\n",
      "epoch 52, loss: 103197, train: 0.512285, validation: 0.509157\n",
      "epoch 53, loss: 103190, train: 0.512514, validation: 0.50943\n",
      "epoch 54, loss: 103172, train: 0.511989, validation: 0.509975\n",
      "epoch 55, loss: 103169, train: 0.512904, validation: 0.509811\n",
      "epoch 56, loss: 103156, train: 0.51248, validation: 0.510956\n",
      "epoch 57, loss: 103142, train: 0.51215, validation: 0.511174\n",
      "epoch 58, loss: 103132, train: 0.511854, validation: 0.511011\n",
      "epoch 59, loss: 103121, train: 0.511605, validation: 0.511229\n",
      "epoch 60, loss: 103113, train: 0.511572, validation: 0.512046\n",
      "epoch 61, loss: 103105, train: 0.512002, validation: 0.51161\n",
      "epoch 62, loss: 103093, train: 0.511356, validation: 0.51161\n",
      "epoch 63, loss: 103084, train: 0.511404, validation: 0.511392\n",
      "epoch 64, loss: 103075, train: 0.511511, validation: 0.511065\n",
      "epoch 65, loss: 103068, train: 0.511693, validation: 0.511665\n",
      "epoch 66, loss: 103058, train: 0.511464, validation: 0.511011\n",
      "epoch 67, loss: 103050, train: 0.511726, validation: 0.511065\n",
      "epoch 68, loss: 103043, train: 0.511175, validation: 0.510247\n",
      "epoch 69, loss: 103033, train: 0.51135, validation: 0.510356\n",
      "epoch 70, loss: 103024, train: 0.511215, validation: 0.510575\n",
      "epoch 71, loss: 103016, train: 0.511181, validation: 0.510356\n",
      "epoch 72, loss: 103004, train: 0.511242, validation: 0.510629\n",
      "epoch 73, loss: 102996, train: 0.511289, validation: 0.510466\n",
      "epoch 74, loss: 102988, train: 0.511255, validation: 0.510302\n",
      "epoch 75, loss: 102973, train: 0.512312, validation: 0.512155\n",
      "epoch 76, loss: 102966, train: 0.511303, validation: 0.510575\n",
      "epoch 77, loss: 102954, train: 0.511471, validation: 0.511011\n",
      "epoch 78, loss: 102934, train: 0.514439, validation: 0.514172\n",
      "epoch 79, loss: 102924, train: 0.512837, validation: 0.512319\n",
      "epoch 80, loss: 102898, train: 0.51954, validation: 0.517279\n",
      "epoch 81, loss: 102878, train: 0.517366, validation: 0.516025\n",
      "epoch 82, loss: 102852, train: 0.518079, validation: 0.516189\n",
      "epoch 83, loss: 102828, train: 0.52495, validation: 0.517606\n",
      "epoch 84, loss: 102805, train: 0.521781, validation: 0.518151\n",
      "epoch 85, loss: 102779, train: 0.523328, validation: 0.518587\n",
      "epoch 86, loss: 102745, train: 0.525805, validation: 0.519514\n",
      "epoch 87, loss: 102709, train: 0.528053, validation: 0.517442\n",
      "epoch 88, loss: 102694, train: 0.530179, validation: 0.518914\n",
      "epoch 89, loss: 102689, train: 0.527292, validation: 0.518478\n",
      "epoch 90, loss: 102661, train: 0.530563, validation: 0.520931\n",
      "epoch 91, loss: 102662, train: 0.528288, validation: 0.519132\n",
      "epoch 92, loss: 102630, train: 0.530967, validation: 0.521858\n",
      "epoch 93, loss: 102623, train: 0.530994, validation: 0.519732\n",
      "epoch 94, loss: 102616, train: 0.530603, validation: 0.520222\n",
      "epoch 95, loss: 102595, train: 0.532037, validation: 0.52431\n",
      "epoch 96, loss: 102596, train: 0.531774, validation: 0.52382\n",
      "epoch 97, loss: 102587, train: 0.532925, validation: 0.524147\n",
      "epoch 98, loss: 102584, train: 0.532259, validation: 0.52322\n",
      "epoch 99, loss: 102571, train: 0.532151, validation: 0.524419\n",
      "epoch 100, loss: 102565, train: 0.532615, validation: 0.52491\n",
      "epoch 101, loss: 102562, train: 0.53234, validation: 0.52382\n",
      "epoch 102, loss: 102568, train: 0.532023, validation: 0.523874\n",
      "epoch 103, loss: 102556, train: 0.532602, validation: 0.525183\n",
      "epoch 104, loss: 102555, train: 0.533712, validation: 0.526055\n",
      "epoch 105, loss: 102547, train: 0.532838, validation: 0.524419\n",
      "epoch 106, loss: 102546, train: 0.532535, validation: 0.525401\n",
      "epoch 107, loss: 102551, train: 0.533786, validation: 0.52709\n",
      "epoch 108, loss: 102548, train: 0.531969, validation: 0.523765\n",
      "epoch 109, loss: 102534, train: 0.533605, validation: 0.524747\n",
      "epoch 110, loss: 102527, train: 0.533248, validation: 0.525619\n",
      "epoch 111, loss: 102528, train: 0.534298, validation: 0.527145\n",
      "epoch 112, loss: 102523, train: 0.532582, validation: 0.525673\n",
      "epoch 113, loss: 102509, train: 0.533376, validation: 0.52769\n",
      "epoch 114, loss: 102513, train: 0.533504, validation: 0.5266\n",
      "epoch 115, loss: 102516, train: 0.534904, validation: 0.527254\n",
      "epoch 116, loss: 102515, train: 0.533376, validation: 0.526981\n",
      "epoch 117, loss: 102507, train: 0.534877, validation: 0.526273\n",
      "epoch 118, loss: 102503, train: 0.534998, validation: 0.525946\n",
      "epoch 119, loss: 102497, train: 0.534931, validation: 0.525946\n",
      "epoch 120, loss: 102507, train: 0.534998, validation: 0.529598\n",
      "epoch 121, loss: 102487, train: 0.53448, validation: 0.525401\n",
      "epoch 122, loss: 102493, train: 0.533941, validation: 0.526818\n",
      "epoch 123, loss: 102491, train: 0.53598, validation: 0.531015\n",
      "epoch 124, loss: 102474, train: 0.535085, validation: 0.526109\n",
      "epoch 125, loss: 102473, train: 0.535509, validation: 0.527199\n",
      "epoch 126, loss: 102468, train: 0.535893, validation: 0.526709\n",
      "epoch 127, loss: 102471, train: 0.536882, validation: 0.528835\n",
      "epoch 128, loss: 102467, train: 0.535146, validation: 0.526055\n",
      "epoch 129, loss: 102459, train: 0.535745, validation: 0.526981\n",
      "epoch 130, loss: 102468, train: 0.534843, validation: 0.527254\n",
      "epoch 131, loss: 102457, train: 0.535677, validation: 0.527853\n",
      "epoch 132, loss: 102445, train: 0.536337, validation: 0.528671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 133, loss: 102452, train: 0.536438, validation: 0.52769\n",
      "epoch 134, loss: 102435, train: 0.537205, validation: 0.529107\n",
      "epoch 135, loss: 102438, train: 0.537575, validation: 0.532977\n",
      "epoch 136, loss: 102427, train: 0.536324, validation: 0.528235\n",
      "epoch 137, loss: 102423, train: 0.537266, validation: 0.528072\n",
      "epoch 138, loss: 102421, train: 0.537367, validation: 0.528562\n",
      "epoch 139, loss: 102411, train: 0.537394, validation: 0.530034\n",
      "epoch 140, loss: 102417, train: 0.536882, validation: 0.527853\n",
      "epoch 141, loss: 102415, train: 0.538766, validation: 0.530906\n",
      "epoch 142, loss: 102409, train: 0.539507, validation: 0.53325\n",
      "epoch 143, loss: 102397, train: 0.538396, validation: 0.531015\n",
      "epoch 144, loss: 102396, train: 0.538955, validation: 0.532596\n",
      "epoch 145, loss: 102387, train: 0.539406, validation: 0.532378\n",
      "epoch 146, loss: 102391, train: 0.539534, validation: 0.532105\n",
      "epoch 147, loss: 102376, train: 0.539345, validation: 0.531233\n",
      "epoch 148, loss: 102363, train: 0.539399, validation: 0.531887\n",
      "epoch 149, loss: 102368, train: 0.539338, validation: 0.526981\n",
      "epoch 150, loss: 102359, train: 0.539453, validation: 0.53096\n",
      "epoch 151, loss: 102356, train: 0.539493, validation: 0.527744\n",
      "epoch 152, loss: 102347, train: 0.540658, validation: 0.532378\n",
      "epoch 153, loss: 102351, train: 0.541216, validation: 0.531887\n",
      "epoch 154, loss: 102325, train: 0.540395, validation: 0.531615\n",
      "epoch 155, loss: 102315, train: 0.540866, validation: 0.531778\n",
      "epoch 156, loss: 102310, train: 0.54133, validation: 0.531669\n",
      "epoch 157, loss: 102297, train: 0.542562, validation: 0.531615\n",
      "epoch 158, loss: 102289, train: 0.542024, validation: 0.530143\n",
      "epoch 159, loss: 102280, train: 0.543417, validation: 0.531124\n",
      "epoch 160, loss: 102261, train: 0.544063, validation: 0.531069\n",
      "epoch 161, loss: 102262, train: 0.544749, validation: 0.532705\n",
      "epoch 162, loss: 102263, train: 0.541929, validation: 0.527635\n",
      "epoch 163, loss: 102223, train: 0.54524, validation: 0.530633\n",
      "epoch 164, loss: 102207, train: 0.545994, validation: 0.531724\n",
      "epoch 165, loss: 102174, train: 0.546654, validation: 0.531342\n",
      "epoch 166, loss: 102142, train: 0.547636, validation: 0.529053\n",
      "epoch 167, loss: 102115, train: 0.548498, validation: 0.528344\n",
      "epoch 168, loss: 102092, train: 0.550315, validation: 0.530361\n",
      "epoch 169, loss: 102047, train: 0.551014, validation: 0.530088\n",
      "epoch 170, loss: 102012, train: 0.552165, validation: 0.528998\n",
      "epoch 171, loss: 101956, train: 0.553457, validation: 0.529598\n",
      "epoch 172, loss: 101889, train: 0.55516, validation: 0.530034\n",
      "epoch 173, loss: 101818, train: 0.557838, validation: 0.528835\n",
      "epoch 174, loss: 101770, train: 0.557993, validation: 0.526164\n",
      "epoch 175, loss: 101675, train: 0.562509, validation: 0.526763\n",
      "epoch 176, loss: 101553, train: 0.56407, validation: 0.524474\n",
      "epoch 177, loss: 101420, train: 0.566049, validation: 0.525673\n",
      "epoch 178, loss: 101331, train: 0.566722, validation: 0.521095\n",
      "epoch 179, loss: 101189, train: 0.570383, validation: 0.519786\n",
      "epoch 180, loss: 101057, train: 0.573122, validation: 0.51717\n",
      "epoch 181, loss: 100930, train: 0.575793, validation: 0.517006\n",
      "epoch 182, loss: 100817, train: 0.576426, validation: 0.517388\n",
      "epoch 183, loss: 100669, train: 0.578936, validation: 0.518206\n",
      "epoch 184, loss: 100577, train: 0.581096, validation: 0.516461\n",
      "epoch 185, loss: 100444, train: 0.582886, validation: 0.514608\n",
      "epoch 186, loss: 100355, train: 0.583802, validation: 0.511338\n",
      "epoch 187, loss: 100223, train: 0.585255, validation: 0.511937\n",
      "epoch 188, loss: 100166, train: 0.586527, validation: 0.513845\n",
      "epoch 189, loss: 99982.5, train: 0.589132, validation: 0.514172\n",
      "epoch 190, loss: 99893, train: 0.589939, validation: 0.511937\n",
      "epoch 191, loss: 99850.4, train: 0.590639, validation: 0.512973\n",
      "epoch 192, loss: 99731.6, train: 0.593304, validation: 0.513736\n",
      "epoch 193, loss: 99687.1, train: 0.593365, validation: 0.514336\n",
      "epoch 194, loss: 99585.8, train: 0.594441, validation: 0.510629\n",
      "epoch 195, loss: 99477.4, train: 0.596178, validation: 0.516734\n",
      "epoch 196, loss: 99437.1, train: 0.597113, validation: 0.512264\n",
      "epoch 197, loss: 99347.5, train: 0.598748, validation: 0.5127\n",
      "epoch 198, loss: 99276.2, train: 0.59786, validation: 0.511065\n",
      "epoch 199, loss: 99150.9, train: 0.600774, validation: 0.511283\n",
      "epoch 200, loss: 99035.5, train: 0.601178, validation: 0.511992\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[148595,80]\n\t [[Node: hero_layers_1/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_x_radiant_0_5, hero_layers_1/Variable/read)]]\n\nCaused by op 'hero_layers_1/MatMul_1', defined at:\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-a14b90552de3>\", line 5, in <module>\n    team2_layer1 = tf.nn.relu(tf.matmul(x_team2, W_hero1) + b_hero1)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1816, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1217, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[148595,80]\n\t [[Node: hero_layers_1/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_x_radiant_0_5, hero_layers_1/Variable/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[148595,80]\n\t [[Node: hero_layers_1/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_x_radiant_0_5, hero_layers_1/Variable/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-cf0acf360cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#log every epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mvalidation_loss\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \"\"\"\n\u001b[1;32m--> 606\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3926\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3927\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3928\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[148595,80]\n\t [[Node: hero_layers_1/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_x_radiant_0_5, hero_layers_1/Variable/read)]]\n\nCaused by op 'hero_layers_1/MatMul_1', defined at:\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-a14b90552de3>\", line 5, in <module>\n    team2_layer1 = tf.nn.relu(tf.matmul(x_team2, W_hero1) + b_hero1)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1816, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1217, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[148595,80]\n\t [[Node: hero_layers_1/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_x_radiant_0_5, hero_layers_1/Variable/read)]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):    \n",
    "    for mini_batch in get_batches(train):\n",
    "        mini_batch_feed = get_data_feed(mini_batch, 0.5, 0.5)   \n",
    "        train_step.run(feed_dict = mini_batch_feed)\n",
    "    \n",
    "    #log every epoch\n",
    "    train_loss          = loss.eval(feed_dict = train_feed)\n",
    "    validation_loss     = loss.eval(feed_dict = validation_feed)\n",
    "\n",
    "    train_accuracy      = accuracy.eval(feed_dict = train_feed)\n",
    "    validation_accuracy = accuracy.eval(feed_dict = validation_feed)\n",
    "\n",
    "    print(\"epoch %d, loss: %g, train: %g, validation: %g\"% (i, train_loss, train_accuracy, validation_accuracy)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy.eval(feed_dict=test_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
