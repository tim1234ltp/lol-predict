{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "\n",
    "This is the notebook for building PredictionByHero model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load and randomise data\n",
    "dataset = pd.read_csv('heroSelect.csv', index_col = 0)\n",
    "dataset = dataset.take(np.random.permutation(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split dependent/independent variables\n",
    "x = dataset.drop('team1Win', axis=1)\n",
    "y = dataset['team1Win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset (183452, 137)\n",
      "train: (148595, 137) validation: (18346, 137) test: (16511, 137)\n"
     ]
    }
   ],
   "source": [
    "print('dataset', dataset.shape)\n",
    "dataset, validation = train_test_split(dataset, test_size = 0.1)\n",
    "train, test = train_test_split(dataset, test_size = 0.1)\n",
    "print('train:', train.shape, 'validation:', validation.shape, 'test:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input/output placeholders\n",
    "x_teamComp = tf.placeholder(\"float\", shape=[None, 136], name='x_teamComp')\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 2], name='y_true')\n",
    "\n",
    "#we'll use dropout layers for regularisation which need a keep probability\n",
    "keep_prob1 = tf.placeholder(\"float\", name='keep_prob1')\n",
    "keep_prob2 = tf.placeholder(\"float\", name='keep_prob2')\n",
    "\n",
    "#there doesn't seem to be any other way to differenciate train and validation summaries for TensorBoard\n",
    "loss_name     = tf.placeholder(\"string\", name='loss_name')\n",
    "accuracy_name = tf.placeholder(\"string\", name='accuracy_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight init for fully connected layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_weight_bias(in_size, out_size):\n",
    "    initial_weight = tf.truncated_normal([in_size, out_size], stddev=0.3, mean=0.0)\n",
    "    initial_bias = tf.constant(0.2, shape=[out_size])\n",
    "    return tf.Variable(initial_weight), tf.Variable(initial_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first hero layer\n",
    "with tf.name_scope(\"hero_layers_1\") as scope:\n",
    "    W_hero1, b_hero1 = fc_weight_bias(136,100)      \n",
    "    #note that team1 layer and team2 layer use the same weights and biases\n",
    "    hero_layer1 = tf.nn.relu(tf.matmul(x_teamComp, W_hero1) + b_hero1)\n",
    "    \n",
    "with tf.name_scope(\"hidden_layer_1\") as scope:\n",
    "    W_hidden1, b_hidden1 = fc_weight_bias(100,80)    \n",
    "    h_hidden1 = tf.nn.relu(tf.matmul(hero_layer1, W_hidden1) + b_hidden1)\n",
    "    h_drop1 = tf.nn.dropout(h_hidden1, keep_prob1)\n",
    "\n",
    "with tf.name_scope(\"hidden_layer_2\") as scope:\n",
    "    W_hidden2, b_hidden2 = fc_weight_bias(80,60)    \n",
    "    h_hidden2 = tf.nn.relu(tf.matmul(h_drop1, W_hidden2) + b_hidden2)\n",
    "    h_drop2 =  tf.nn.dropout(h_hidden2, keep_prob2)\n",
    "    \n",
    "with tf.name_scope(\"hidden_layer_3\") as scope:\n",
    "    W_hidden3, b_hidden3 = fc_weight_bias(60,20)    \n",
    "    h_hidden3 = tf.nn.relu(tf.matmul(h_drop2, W_hidden3) + b_hidden3)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"output_layer\") as scope:\n",
    "    W_hidden4, b_hidden4 = fc_weight_bias(20,2)    \n",
    "    y = tf.nn.softmax(tf.matmul(h_hidden3, W_hidden4) + b_hidden4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss_calculations\") as scope:\n",
    "    cross_entropy = -tf.reduce_sum(y_ * tf.log(y+1e-8))\n",
    "    weights_sum   = tf.add_n([tf.nn.l2_loss(variable) for variable in tf.global_variables()])\n",
    "    loss          = cross_entropy + weights_sum\n",
    "    mean_loss     = tf.reduce_mean(loss)\n",
    "\n",
    "with tf.name_scope(\"trainer\") as scope:\n",
    "    train_step    = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy_calculations\") as scope:\n",
    "    correct  = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#summarize the accuracy and loss \n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "mean_loss_summary = tf.summary.scalar('loss', mean_loss)\n",
    "\n",
    "#summarize the distribution of output values\n",
    "y_hist = tf.summary.histogram(\"y\", y)\n",
    "\n",
    "#gather all summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter(\"logdir\", sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFeed Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_feed(dataset, kp1=0.7, kp2=0.7, loss_str='loss', accuracy_str='accuracy'):\n",
    "    team_data = dataset.ix[:,:136]\n",
    "    winners = pd.get_dummies(dataset['team1Win'])\n",
    "    return {\n",
    "        x_teamComp: team_data,\n",
    "        y_: winners,\n",
    "        loss_name: loss_str,\n",
    "        accuracy_name: accuracy_str,\n",
    "        keep_prob1: kp1,\n",
    "        keep_prob2: kp2\n",
    "    }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_feed      = get_data_feed(train,      loss_str = 'loss_train',      accuracy_str = 'accuracy_train')\n",
    "validation_feed = get_data_feed(validation, loss_str = 'loss_validation', accuracy_str = 'accuracy_validation')\n",
    "test_feed       = get_data_feed(test,       loss_str = 'loss_test',       accuracy_str = 'accuracy_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(dataset, batch_size=1500): #1500 is about 1% of the entire training sets\n",
    "    #randomise before every epoch\n",
    "    dataset = dataset.take(np.random.permutation(len(dataset)))\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(dataset):\n",
    "        yield dataset[i : i + batch_size]\n",
    "        i = i + batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tim12\\.conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 707.227, train: 1, validation: 1\n",
      "epoch 1, loss: 693.604, train: 1, validation: 1\n",
      "epoch 2, loss: 680.441, train: 0.999993, validation: 1\n",
      "epoch 3, loss: 668.539, train: 1, validation: 1\n",
      "epoch 4, loss: 654.392, train: 1, validation: 1\n",
      "epoch 5, loss: 640.228, train: 0.999993, validation: 1\n",
      "epoch 6, loss: 625.604, train: 1, validation: 1\n",
      "epoch 7, loss: 611.769, train: 1, validation: 1\n",
      "epoch 8, loss: 598.422, train: 1, validation: 1\n",
      "epoch 9, loss: 583.294, train: 1, validation: 1\n",
      "epoch 10, loss: 569.719, train: 1, validation: 1\n",
      "epoch 11, loss: 554.878, train: 1, validation: 1\n",
      "epoch 12, loss: 541.302, train: 1, validation: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ecfcaa30c480>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#log every epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mvalidation_loss\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \"\"\"\n\u001b[1;32m--> 606\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3926\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3927\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3928\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):    \n",
    "    for mini_batch in get_batches(train):\n",
    "        mini_batch_feed = get_data_feed(mini_batch, 0.5, 0.5)   \n",
    "        train_step.run(feed_dict = mini_batch_feed)\n",
    "    \n",
    "    #log every epoch\n",
    "    train_loss          = loss.eval(feed_dict = train_feed)\n",
    "    validation_loss     = loss.eval(feed_dict = validation_feed)\n",
    "\n",
    "    train_accuracy      = accuracy.eval(feed_dict = train_feed)\n",
    "    validation_accuracy = accuracy.eval(feed_dict = validation_feed)\n",
    "\n",
    "    train_summary_str      = merged.eval(feed_dict = train_feed)\n",
    "    validation_summary_str = merged.eval(feed_dict = validation_feed)                \n",
    "\n",
    "    writer.add_summary(train_summary_str, i)\n",
    "    writer.add_summary(validation_summary_str, i)\n",
    "    print(\"epoch %d, loss: %g, train: %g, validation: %g\"% (i, train_loss, train_accuracy, validation_accuracy)) \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.eval(feed_dict=test_feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
